{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     \n",
    "import pandas as pd    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_norm</th>\n",
       "      <th>stemmed_keyword</th>\n",
       "      <th>disaster_asc_coeff</th>\n",
       "      <th>length_norm</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>earthquak</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>fire</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>36</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>evacu</td>\n",
       "      <td>0.767241</td>\n",
       "      <td>69</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "      <td>evacu</td>\n",
       "      <td>0.767241</td>\n",
       "      <td>49</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>fire</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>52</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                           text_norm stemmed_keyword  \\\n",
       "0          deed reason earthquak may allah forgiv us       earthquak   \n",
       "1               forest fire near la rong sask canada            fire   \n",
       "2  resid ask shelter place notifi offic evacu she...           evacu   \n",
       "3  13000 peopl receiv wildfir evacu order california           evacu   \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...            fire   \n",
       "\n",
       "   disaster_asc_coeff  length_norm    neg    neu  pos  \n",
       "0            0.785714           41  0.000  1.000  0.0  \n",
       "1            0.386364           36  0.286  0.714  0.0  \n",
       "2            0.767241           69  0.000  1.000  0.0  \n",
       "3            0.767241           49  0.000  1.000  0.0  \n",
       "4            0.386364           52  0.000  1.000  0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset containing normalized text as well as the additional features obtained from notebook 1\n",
    "\n",
    "df_train = pd.read_csv('Data\\\\disaster_tweets_kaggle\\\\train_clean_add_feat.csv', index_col = False)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part we will compare results using the original text with minimum processing vs the normalized text \n",
    "df_orig = df_train[['text', 'target']]\n",
    "df_norm = df_train[['text_norm', 'target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Training with Normalized Text Data\n",
    "For this text data, stop words, symbols, urls and punctuations have been removed. In addition, the words have also been stemmed. This means that the vocabulariy will be smaller; however, any contextual information provided by stop words will be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessingcessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(df_norm['text_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [1179],\n",
       " [1179],\n",
       " [],\n",
       " [],\n",
       " [857],\n",
       " [1179],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [600],\n",
       " [],\n",
       " [1179],\n",
       " [],\n",
       " [857],\n",
       " [],\n",
       " [2014],\n",
       " [5121],\n",
       " [103],\n",
       " [],\n",
       " [11038],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [2569],\n",
       " [2569],\n",
       " [],\n",
       " [2014],\n",
       " [],\n",
       " [1344],\n",
       " [],\n",
       " [857],\n",
       " [1941],\n",
       " [],\n",
       " [1257],\n",
       " [],\n",
       " [103],\n",
       " []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(df_norm['text_norm'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 19542\n",
      "Max sequence length: 25\n"
     ]
    }
   ],
   "source": [
    "# get max length to pad sequences and abd vocab size\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "MAX_LENGTH = max([len(text.split()) for text in df_norm['text_norm']])\n",
    "print(f'Vocab size: {VOCAB_SIZE}')\n",
    "print(f'Max sequence length: {MAX_LENGTH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will set embedding dimension to be 100\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_norm['text_norm']\n",
    "y = df_norm['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5797    accionempresa chinaûª stock market crash summ...\n",
       "3232    feel engulf low selfimag take quiz httptcoykvs...\n",
       "5145    mccainenl think spectacular look stonewal riot...\n",
       "4911    magic citi mayhem kissimme adventur aug 5 2015...\n",
       "253     us nation park servic tonto nation forest stop...\n",
       "Name: text_norm, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text data\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply padding\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen = MAX_LENGTH, padding = 'post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen = MAX_LENGTH, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tnsrflow]",
   "language": "python",
   "name": "conda-env-tnsrflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
