{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-27T04:03:41.285938Z",
     "iopub.status.busy": "2023-01-27T04:03:41.285524Z",
     "iopub.status.idle": "2023-01-27T04:03:41.330567Z",
     "shell.execute_reply": "2023-01-27T04:03:41.328828Z",
     "shell.execute_reply.started": "2023-01-27T04:03:41.285849Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "'''\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:09:58.110987Z",
     "iopub.status.busy": "2023-01-27T04:09:58.110517Z",
     "iopub.status.idle": "2023-01-27T04:09:58.183424Z",
     "shell.execute_reply": "2023-01-27T04:09:58.182623Z",
     "shell.execute_reply.started": "2023-01-27T04:09:58.110953Z"
    }
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "# train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "# test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "train = pd.read_csv('Data/disaster_tweets_kaggle/train.csv')\n",
    "test = pd.read_csv('Data/disaster_tweets_kaggle/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:10:05.073998Z",
     "iopub.status.busy": "2023-01-27T04:10:05.073454Z",
     "iopub.status.idle": "2023-01-27T04:10:05.091998Z",
     "shell.execute_reply": "2023-01-27T04:10:05.090944Z",
     "shell.execute_reply.started": "2023-01-27T04:10:05.073961Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:10:39.736951Z",
     "iopub.status.busy": "2023-01-27T04:10:39.736481Z",
     "iopub.status.idle": "2023-01-27T04:10:39.764397Z",
     "shell.execute_reply": "2023-01-27T04:10:39.763590Z",
     "shell.execute_reply.started": "2023-01-27T04:10:39.736913Z"
    }
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:12:13.712931Z",
     "iopub.status.busy": "2023-01-27T04:12:13.712423Z",
     "iopub.status.idle": "2023-01-27T04:12:14.196225Z",
     "shell.execute_reply": "2023-01-27T04:12:14.194911Z",
     "shell.execute_reply.started": "2023-01-27T04:12:13.712888Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:13:31.223992Z",
     "iopub.status.busy": "2023-01-27T04:13:31.223551Z",
     "iopub.status.idle": "2023-01-27T04:13:31.238067Z",
     "shell.execute_reply": "2023-01-27T04:13:31.237082Z",
     "shell.execute_reply.started": "2023-01-27T04:13:31.223961Z"
    }
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:14:56.030477Z",
     "iopub.status.busy": "2023-01-27T04:14:56.030027Z",
     "iopub.status.idle": "2023-01-27T04:14:56.197094Z",
     "shell.execute_reply": "2023-01-27T04:14:56.196046Z",
     "shell.execute_reply.started": "2023-01-27T04:14:56.030440Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's check if the training data is balanced\n",
    "sns.countplot(x = 'target', data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:15:25.902735Z",
     "iopub.status.busy": "2023-01-27T04:15:25.902329Z",
     "iopub.status.idle": "2023-01-27T04:15:26.061814Z",
     "shell.execute_reply": "2023-01-27T04:15:26.060858Z",
     "shell.execute_reply.started": "2023-01-27T04:15:25.902706Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualizing null values\n",
    "sns.heatmap(train.isnull(),yticklabels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many null values for location therefore the column will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:17:30.635273Z",
     "iopub.status.busy": "2023-01-27T04:17:30.634789Z",
     "iopub.status.idle": "2023-01-27T04:17:30.804120Z",
     "shell.execute_reply": "2023-01-27T04:17:30.803057Z",
     "shell.execute_reply.started": "2023-01-27T04:17:30.635229Z"
    }
   },
   "outputs": [],
   "source": [
    "train.drop(['id', 'location'], axis = 1, inplace = True)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:23:32.119840Z",
     "iopub.status.busy": "2023-01-27T04:23:32.119404Z",
     "iopub.status.idle": "2023-01-27T04:23:32.138955Z",
     "shell.execute_reply": "2023-01-27T04:23:32.137752Z",
     "shell.execute_reply.started": "2023-01-27T04:23:32.119802Z"
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('keyword').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords can be helpful in classification, we will drop the rows with missing keywords as there are very few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:26:43.412731Z",
     "iopub.status.busy": "2023-01-27T04:26:43.412250Z",
     "iopub.status.idle": "2023-01-27T04:26:43.431400Z",
     "shell.execute_reply": "2023-01-27T04:26:43.430443Z",
     "shell.execute_reply.started": "2023-01-27T04:26:43.412698Z"
    }
   },
   "outputs": [],
   "source": [
    "train.dropna(axis = 0, inplace = True)\n",
    "train.reset_index(drop = True, inplace = True)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T04:19:33.740851Z",
     "iopub.status.busy": "2023-01-27T04:19:33.740397Z",
     "iopub.status.idle": "2023-01-27T04:19:33.758109Z",
     "shell.execute_reply": "2023-01-27T04:19:33.756280Z",
     "shell.execute_reply.started": "2023-01-27T04:19:33.740817Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's check the test data\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide what strategy to use for data processing and what type of a model to use, the training data must be thoroughly analyzed. Firstly, we will check if all keywords are related to disaster and formulate a method to identify which ones are more relevant with the help of the associated target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series of unique keywords\n",
    "pd.Series(train.groupby('keyword').count().index, name = 'keyword')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As most keywords seem to be asscoiated with disasters, we need to perform feature engineering to extract useful information from the distribution of these keywords with respect to the target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing using NLTK\n",
    "Each keyword will be converted into its root form so that keyword like 'blood' and 'bloody' will be considered as the same keywords. Same goes for all keywords that are closely related in the same manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column with stemmed keyword\n",
    "train['stemmed_keyword'] = pd.Series(stemmer.stem(keyword) for keyword in train['keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of tokens given a sentence\n",
    "def get_tokens(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# calculates the disaster association coefficient for a keyword\n",
    "def get_disaster_association(keyword):\n",
    "    \n",
    "    # get the number of times the keyword is associated with a true disaster tweet\n",
    "    target_0_count = (train[train['stemmed_keyword'] == keyword]['target'] == 0).sum()\n",
    "    \n",
    "    # get the number of times the keyword is not associated with a true disaster tweet\n",
    "    target_1_count = (train[train['stemmed_keyword'] == keyword]['target'] == 1).sum()\n",
    "    \n",
    "    # association value = true association count / total\n",
    "    return target_1_count/(target_0_count + target_1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns with the keyword's association to actual disaster tweeets\n",
    "train['disaster_asc_coeff'] = train['stemmed_keyword'].apply(get_disaster_association)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save association oefficients as a dictionary for transformation\n",
    "association_dict = train.groupby('stemmed_keyword')['association_coeff'].mean().sort_values(ascending = False).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Location mentions in Tweet\n",
    "Mentions of a certain area, city or country in the tweet can be useful in identifying whether the tweet is related to disaster or as tweets with a location mentoned are more likely to be linked to disaster. To implement this we will use the <b>geography</b> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install geotext \n",
    "# !pip install geotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geotext\n",
    "from geotext import GeoText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing\n",
    "Next, each text will be processed such that only important information will be retained. Foe example, hyperlinks and hashtag symbols will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['text'][7])\n",
    "print(train['text'][8])\n",
    "print(train['text'][9])\n",
    "print(train['text'][10])\n",
    "print(train['text'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = train['text'][5].split('http')[0].strip()\n",
    "c2 = ''.join(c1.split('#'))\n",
    "' '.join([word for word in c2.split() if '@' not in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_text(raw_text):\n",
    "    \n",
    "    # remove hyperlink\n",
    "    text_c1 = raw_text.split('http')[0].strip()\n",
    "    \n",
    "    # remove hashtags\n",
    "    text_c2 = ''.join(text_c1.split('#'))\n",
    "    \n",
    "    # remove username\n",
    "    text_c3 = ' '.join([word for word in wordtext_c2.split() if '@' not in word])\n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text_c4 = text_c3.lower()\n",
    "    \n",
    "    return text_c4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis (NLTK)\n",
    "Although keywords can be an important factor in this type of classification, the overall sentiment of the tweet can also provide crucial information regarding the nature of the tweet. Although a custom model can be trained to predict sentiment, for this project I am using the <b> NLTK VADER </b> library which relies on a rule based sentiment-analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download([\n",
    "     \"names\",\n",
    "     \"stopwords\",     \"state_union\",\n",
    "     \"twitter_samples\",\n",
    "    \"movie_reviews\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "     \"vader_lexicon\",\n",
    "     \"punkt\",\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary of values\n",
    "def get_sentiment_val(text):\n",
    "    res_dict = sia.polarity_scores(text)\n",
    "    return pd.Series([res_dict['neg'], res_dict['neu'], res_dict['pos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['neg', 'neu', 'pos']] = train['text'].apply(get_sentiment_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed training data to avoid preprocessing time\n",
    "train.drop(['keyword', 'text',\n",
    "            'stemmed_keyword'], axis = 1).to_csv('Data/disaster_tweets_kaggle/cleaned_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('Data/disaster_tweets_kaggle/cleaned_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze distribution with respect to different attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of keyword association coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "sns.kdeplot(df, x = 'association_coeff', hue = 'target')\n",
    "plt.title('Distribution of keyword association coefficient w.r.t. target labels')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above KDE plot shows that distribution of keyword association coefficients for non disaster tweets is more towards the left as opposed to that of disaster related tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of different sentiment tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "sns.kdeplot(df, x = 'pos', hue = 'target')\n",
    "plt.title('Distribution of tweets with positive sentiment w.r.t. target labels')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "sns.kdeplot(df, x = 'neu', hue = 'target')\n",
    "plt.title('Distribution of tweets with neutral sentiment w.r.t. target labels')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "sns.kdeplot(df, x = 'neg', hue = 'target')\n",
    "plt.title('Distribution of tweets with negative sentiment w.r.t. target labels')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see that both targets have similar distribution for all sentiment tags given by NLTK's VADER sentiment analysis package, using these attributes for model training will not be too useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns except 'association coefficient' and those related to sentiment analysis\n",
    "X = train_processed.drop('target', axis = 1)\n",
    "y = train_processed['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "As this is a classification problem, the simplest model that can be used is logistic regression. However, logistic regression only performs well for if classes are linearly separable. For data that is not linearly separable we can use KNN, SVC, Random Forests. All these models will be implemented and the results for each will be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn]",
   "language": "python",
   "name": "conda-env-sklearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
